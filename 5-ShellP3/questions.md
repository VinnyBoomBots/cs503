1. Your shell forks multiple child processes when executing piped commands. How does your implementation ensure that all child processes complete before the shell continues accepting user input? What would happen if you forgot to call waitpid() on all child processes?

This implementation of ensuring that all child processes are complete prior to the shell taking in new input took some trial and error for me to get right. At first, I was completely following the demo code and utilizing a "supervisor" process in my exec_local_cmd_loop() function. However, I soon realized that all the processes were NOT completing prior to new shell input being taken in, and I was thus dealing with issues of my loop never exiting (even when my "exit" command had been provided) -- and I was also dealing with what the console described as "memory leaks." So after some tinkering, I came to realize that the functionality captured in my execute_pipeline() function -- which was taken almost directly from this week's demo code -- was all I needed to ensure that, one, pipes were created and then closed, and two, that all child processes completed prior to the SH_PROMPT being called again (prompting user input). This functionality is accomplished in my code by: using the waitpid() function; using the pid_t pids array as the first parameter; and incrementing that first parameter until it cycles through each command provided on the command line (the function looks like this: waitpid(pids[i], NULL, 0), and its captured within a for-loop that runs for the number of commands provided). And if I didn't have this implementation, my child processes would not end successfully, and there would be, as I personally dealt with, errors of continuous looping, memory leaks -- and the general confusion that occurs when a parent doesn't wait for a child to end, creating "zombie" processes. I speak from experience when I say that this is not only technically unsound, but also remarkably annoying to debug. 

2. The dup2() function is used to redirect input and output file descriptors. Explain why it is necessary to close unused pipe ends after calling dup2(). What could go wrong if you leave pipes open?

After doing a little Internet research -- and also going off of the lecture slides from last week that talked about "cleaning up" unused file descriptors -- it seems as if closing pipe ends just simplifies things across the board: It rids your program of resources you're not using, and it avoids allocating memory to unused entities. However, in small doses unclosed pipe ends are probably not that big of deal -- and Professor Lilley even stated that for the 5-ShellP3 assignment this week, we wouldn't be "bit" by this uncleanliness (if it did in fact occur). So why should we prioritize closing pipes? Well, it also seems that unclosed pipes complicate the transition for certain commands transitioning from reading input to writing output -- because the end-of-file signals that aid in this transition can fail without closed pipe ends, causing programs to stall and spin their tires, so to speak (which DID happen to me, so I speak from experience when I say that it is very annoying). So not closing pipes may not necessarily "bite" you every time -- but the more complicated the commands become, the more it seems certain that unclosed pipes will hurt your shell's functionality in the long run. 

3. Your shell recognizes built-in commands (cd, exit, dragon). Unlike external commands, built-in commands do not require execvp(). Why is cd implemented as a built-in rather than an external command? What challenges would arise if cd were implemented as an external process?

I would think that the cd command ought to be a built-in command because the directory you're working in at the time you run your shell is your "working directory" -- and if you call a child process for an external command, whenever that command completes it will not affect the actual "working directory," because it would only be changing the directory of the child process (which has since terminated). Calling cd is a way to change your directory -- and have it be changed for the next command -- and the only way to ensure that that change actually occurs and sticks is by calling it in the parent process. 

4. Currently, your shell supports a fixed number of piped commands (CMD_MAX). How would you modify your implementation to allow an arbitrary number of piped commands while still handling memory allocation efficiently? What trade-offs would you need to consider?

I guess what you could do is continue to allocate memory dynamically, but do so according to an unknown number of commands. Perhaps you could declare a char *cmd_buff and then call malloc(), but as a parameter you could pass in a constant that is multiplied by the number of commands provided by the user (like, say, malloc(MEMORY * num_commands)). This way malloc() could allocate memory and adjust the size of that memory to the number of commands provided by the user. This could possibly work -- that is, until all available memory is monopolized. In my estimation, one of the things that would need to be considered is the utility of having an unlimited amount of memory allocated for certain commands -- because just simplistically, is it really realistic that a user needs to pipe 65+ commands together? Does a reasonable use case for that exist? And is that use case even worth preparing for -- and, more importantly, accomodating? There will always be edge cases, but are those edge cases worth planning FOR, not just around? I think one needs to weigh the trade-offs of close-to-unlimited memory allocation against the reasonableness of such large requests -- and the ability of a shell to even process requests of that nature. 
